x = x %>% select(-c(zip_code,
id,
member_id,
emp_title,
title,
url,
desc))
# Keep only the features that are available to us before someone invested on the loan.
# Features that were not available before loan disbursement:
# total_pymnt
# total_pymnt_inv
# total_rec_prncp,
# total_rec_int,
# total_rec_late_fee,
# recoveries,
# collection_recovery_fee,
# last_pymnt_d,
# last_pymnt_amnt,
x = x %>% select(-c(issue_d,
total_pymnt,
total_pymnt_inv,
total_rec_prncp,
total_rec_int,
total_rec_late_fee,
recoveries,
collection_recovery_fee,
last_pymnt_d,
last_pymnt_amnt))
##### Dates converted to factors for now
# Are you guys cool with converting some of these dates to numeric values where we can? # I think they could be a little more useful this way instead of just leaving them out of the model or converting them all as factors.
#issue_d: date
#last_credit_pull_d: date
#earliest_cr_line: date
# Converting employment length to number of years they have been employed.
x$emp_length[which(x$emp_length == "10+ years")] = 10
x$emp_length[which(x$emp_length == "< 1 year")] = 0
x$emp_length[which(x$emp_length == "n/a")] = NA
x$emp_length = sub(" years", "", x$emp_length)
x$emp_length = sub(" year", "", x$emp_length)
x$emp_length = as.numeric(x$emp_length)
#*** professor said we should remove this **#
#Converted issue_d to the year that the loan was issued
#x$issue_d = sub("-.*", "", x$issue_d)
#x$issue_d = sub("-.*", "", x$issue_d)
#x$issue_d = sub("-.*", "", x$issue_d)
#This code will convert earliest_cr_line to the number of years since their earliest credit line.
x$earliest_cr_line = sub("-[[:alpha:]]+","", x$earliest_cr_line)
x$earliest_cr_line = sub(".*-", "", x$earliest_cr_line) %>% as.numeric(.)
x$earliest_cr_line = ifelse(x$earliest_cr_line <= 8, 8-x$earliest_cr_line, x$earliest_cr_line)
x$earliest_cr_line = as.character(x$earliest_cr_line)
x$earliest_cr_line = sub(" ", "", x$earliest_cr_line) %>% as.numeric(.)
x$earliest_cr_line = ifelse(x$earliest_cr_line > 8, paste(19,x$earliest_cr_line), x$earliest_cr_line)
x$earliest_cr_line = sub(" ", "", x$earliest_cr_line)
x$earliest_cr_line = as.numeric(x$earliest_cr_line)
x$earliest_cr_line = ifelse(x$earliest_cr_line > 8, 2008 - x$earliest_cr_line, x$earliest_cr_line)
#This converts last_credit_pull_d to the year of their last credit pull.
x$last_credit_pull_d = sub("-.*", "", x$last_credit_pull_d)
#Gets rid of the percent signs for revol_util and int_rate and converts them to numeric
x$revol_util = as.numeric(sub("%","",x$revol_util))
x$int_rate = as.numeric(sub("%","",x$int_rate))
#convert data types
#training data
xQual = x %>% select(c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)
xQuan = x %>% select(-c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% as.data.frame(.)
#final result
table(sapply(cbind(xQual[1,],xQuan[1,]),class)) # number of features per data type
str(xQual)
str(xQuan)
str(y)
print("train")
(skewed= apply(xQuan, 2, skewness, na.rm=TRUE))
#center and scale
xQuanPCA = xQuan %>% preProcess(.) %>% predict(newdata = xQuan)
#Correcting for skewness
skewFeats = names(which(abs(skewed) > 1.5))
xQuanYJ = xQuanPCA %>%
select(abs(contains(skewFeats))) %>%
preProcess(method = 'YeoJohnson', na.rm=TRUE) %>%
predict(xQuanPCA %>% select(contains(skewFeats)))
xQuanNotSkew = xQuanPCA %>% select(!contains(skewFeats))
apply(xQuanYJ, 2, skewness, na.rm=TRUE)
xQuanYJ = cbind(xQuanYJ, xQuanNotSkew)
pcaOut = prcomp(na.omit(xQuanYJ,scale=TRUE,center=TRUE))
xQuanScores = data.frame(pcaOut$x)
ggplot(data = xQuanScores) + geom_point(aes(x = PC1, y = PC2)) +
coord_cartesian(xlim=range(xQuanScores$PC1), ylim=range(xQuanScores$PC2))
(extremeObs = which(xQuanScores$PC2 < -10))
str(xQuan[extremeObs, ])
print('loan amnt')
median(xQuan$loan_amnt)
print('annual inc')
median(xQuan$annual_inc)
print('revolving bal')
median(xQuan$revol_bal)
#Counting the missing data
sapply(xQuan, function(x) sum(is.na(x)))
#mode impute
modeImpute = function(Xqual){
tbl = table(Xqual)
Xqual[is.na(Xqual)] = names(tbl)[which.max(tbl)]
return(Xqual)
}
xQual = xQual %>% mutate(across(.cols=everything(), modeImpute))
#median impute
xQuan = xQuan %>%
preProcess(method = 'medianImpute')%>%
predict(newdata = xQuan)
#Counting the missing data again
sapply(xQual, function(x) sum(is.na(x)))
sapply(xQuan, function(x) sum(is.na(x)))
datacorr = cor(xQuan)
corrplot(datacorr, order= 'hclust', t1.cex= .35)
str(xQuan)
dim(xQuan)
#correlated features
highCorr = findCorrelation(datacorr, .85, verbose=T, names=T)
highCorr
#Removing correlated features
xQuan= select(all_of(xQuan), -any_of((highCorr)))
#Centering and scaling the numeric features (dont want to center and scale dummy variables)
xQuan = xQuan %>% preProcess(.) %>% predict(newdata = xQuan)
#Checking features for imbalanced frequencies. Pg 45 in the book recommends removing features if the fraction of unique values to sample size is low (10%) and the ratio of the most prevalent value to 2nd most prevalent value is large (around 20). if both criteria holds they say it may be advantageous to remove it. On pg 55 they show the code for this.
#I asked about this is the Q&A and he pretty much said either method is fine to use. I am going to try it with and without it.
#str(xQuan)
#freq = nearZeroVar(xQuan, saveMetrics = TRUE)
#xQuan = select_if(xQuan, freq$freqRatio < 30 | freq$percentUnique > 0.10)
#str(xQuan)
xFull = cbind(xQual, xQuan) # combine qualitative and quantitative features
dim(xFull)# total number of observations and features in the training set
names(xFull)# names of remaining features
set.seed(1234567)
#For downsampling, need to make data a data frame and loan_status a factor for the split. I dont think I need to do it if i dont.
#data = as.data.frame(data)
#data$loan_status = as.factor(data$loan_status)
xFull = cbind(y, xFull) %>% as.data.frame()
n=nrow(xFull)
trainingDataIndex = createDataPartition(xFull$loan_status, p = .75, list = FALSE) %>% as.vector(.)
trainingData = xFull[trainingDataIndex, ]
testingData = xFull[-trainingDataIndex, ]
Xtrain = select(trainingData, -loan_status)
Xtest = select(testingData, -loan_status)
Ytrain = select(trainingData, loan_status) %>% unlist() %>% as.factor()
Ytest = select(testingData, loan_status) %>% unlist() %>% as.factor()
rm(trainingData)
rm(testingData)
#splitting the sets into qualitative and quantitative for dummy variable coercion
#training data
XtrainQual = Xtrain %>% select(c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)
XtrainQuan = Xtrain %>% select(-c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% as.data.frame(.)
#test data
XtestQual = Xtest %>% select(c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)
XtestQuan = Xtest %>% select(-c(term,
grade,
sub_grade,
home_ownership,
verification_status,
pymnt_plan,
purpose,
addr_state,
last_credit_pull_d)) %>% as.data.frame(.)
str(XtrainQual)
str(XtestQual)
#pymnt_plan only has 1 level the test set so I am going to remove it since I cannot create dummy variables for it.
XtrainQual = XtrainQual %>% select(-pymnt_plan)
XtestQual = XtestQual %>% select(-pymnt_plan)
#I am filtering for zero variance before I do this because I am getting an error that I can only create dummy variables on factors with 2 or more levels.
#just a note for myself. I tried to create a dummy model for each set but it did not work since the sets have factors with different levels.
dummyModel = dummyVars(~., data = XtrainQual, fullRank=TRUE)
XtrainQualDummy = predict(dummyModel, XtrainQual)
Xtrain = cbind(XtrainQualDummy, XtrainQuan)
XtestQualDummy = predict(dummyModel, XtestQual)
Xtest = cbind(XtestQualDummy, XtestQuan)
dim(Xtrain)
dim(Xtest)
#predictors = select(trainingData, -loan_status)
#outcome = select(trainingData, loan_status) %>% unlist(.)
#trainingData = downSample(x=predictors, y=outcome, yname='loan_status')
XandYtrain = cbind(Xtrain, Ytrain)
XandYtrain = rename(XandYtrain, loan_status=Ytrain)
#downsample function
predictors = select(XandYtrain, -loan_status)
outcome = select(XandYtrain, loan_status) %>% unlist(.)
XandYtrain = downSample(x=predictors, y=outcome, yname='loan_status')
XtrainD = select(XandYtrain, -loan_status)
YtrainD = select(XandYtrain, loan_status) %>% unlist(.)
YtrainRelevel = relevel(YtrainD, ref = 'Default') %>% unlist()
YtestRelevel = relevel(Ytest, ref = 'Default') %>% unlist()
#code for the logistic regression model
outLogistic = train(x = XtrainD, y = YtrainRelevel,
method = 'glm', trControl = trainControl(method='cv',number=10))
summary(outLogistic)
YhattestProb = predict(outLogistic, Xtest, type = 'prob')
#Checking how well calibrated the probabilities are
calibProbs = calibration(YtestRelevel ~ YhattestProb$`Default`)
xyplot(calibProbs)
#Getting default confusion matrix
Yhattest = predict(outLogistic, Xtest, type = 'raw')
confusionMatrixOutLog = confusionMatrix(reference = YtestRelevel, data = Yhattest)
print(confusionMatrixOutLog$table)
print(confusionMatrixOutLog$overall[1:2])
print(confusionMatrixOutLog$byClass[1:2])
#ROC curve
rocCurveLog = roc(Ytest, YhattestProb$`Default`)
plot(rocCurveLog, legacy.axes=TRUE)
rocCurveLog$auc
thresholds = rocCurveLog$thresholds
sort(thresholds)[1:3]
sort(thresholds, decreasing = TRUE)[1:3]
#Getting confusion matrix for particular sensitivity (I dont think we need to do this)
pt5 = which.min(rocCurve$sensitivities >= 0.8)
XtrainMat = as.matrix(XtrainD)
XtestMat = as.matrix(Xtest)
K=10
trainControl = trainControl(method = "repeatedcv", repeats=2, number = K)
set.seed(2)
#Need to increase the range that we allow lamda take on.
tuneGrid = expand.grid('alpha'=c(0,.25,.5,.75, 1),
'lambda' = seq(0.0001, .01, length.out = 100))
elasticOut = train(x = XtrainMat, y = YtrainRelevel,
method = "glmnet",
trControl = trainControl,
tuneGrid = tuneGrid)
elasticOut$bestTune
#Getting the fitted model using the CV minimizing solution
glmnetOut = glmnet(x = XtrainMat, y = YtrainRelevel,
alpha = elasticOut$bestTune$alpha, family = 'binomial',
standardize = FALSE)
probHattestGlmnet = predict(glmnetOut, XtestMat, s=elasticOut$bestTune$lambda,
type = 'response')
YhattestGlmnet = ifelse(probHattestGlmnet > 0.5, 'Fully Paid', 'Default')
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
betaHat[important]
#Confusion Matrix
confusionMatrixEL = table(YhattestGlmnet, Ytest)
confusionMatrixEL
#ROC curve
rocOutEL = roc(response=Ytest, probHattestGlmnet)
plot(rocOutEL)
rocOutEL$auc
table(xQual$last_credit_pull_d)
YhattestGlmnet
glmnetOut
summary(glmnetOut)
names(glmnetOut)
coef(glmnetOut
coef(glmnetOut)
coef(glmnetOut)
coef(glmnetOut)
elasticOut$bestTune
elasticOut$finalModel
elasticOut$finalModel
(betaHat[-1,])
betaHat[-1,]
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
betaHat
table(xQuan$pub_rec_bankruptcies)
#code for the logistic regression model
outLogistic = train(x = XtrainD, y = YtrainRelevel,
method = 'glm', trControl = trainControl(method='cv',number=10))
summary(outLogistic)
YhattestProb = predict(outLogistic, Xtest, type = 'prob')
#Checking how well calibrated the probabilities are
calibProbs = calibration(YtestRelevel ~ YhattestProb$`Default`)
xyplot(calibProbs)
#Getting default confusion matrix
Yhattest = predict(outLogistic, Xtest, type = 'raw')
confusionMatrixOutLog = confusionMatrix(reference = YtestRelevel, data = Yhattest)
print(confusionMatrixOutLog$table)
print(confusionMatrixOutLog$overall[1:2])
print(confusionMatrixOutLog$byClass[1:2])
#ROC curve
rocCurveLog = roc(Ytest, YhattestProb$`Default`)
plot(rocCurveLog, legacy.axes=TRUE)
rocCurveLog$auc
thresholds = rocCurveLog$thresholds
sort(thresholds)[1:3]
sort(thresholds, decreasing = TRUE)[1:3]
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
sort(abs(betaHat[important]))
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
sort((betaHat[important]))
table(xQuan$inq_last_6mths)
table(x$inq_last_6mths)
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
sort((betaHat[important]))
table(x$inq_last_6mths)
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
sort(abs(betaHat[important]))
#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
sort((betaHat[important]))
confusionMatrixOutRF$table
confusionMatrixOutT$table
confusionMatrixOutKNN$table
confusionMatrixOutsvm$table
confusionMatrixOutFDA$table
confusionMatrixEL
rocOutEL$auc
K=10
trainControl = trainControl(method = "repeatedcv", repeats=2, number = K, classProbs=TRUE)
#It says that 'Fully Paid' needs to be a valid r name for it to work so I rename it to 'Paid' here
YtrainT = as.character(YtrainRelevel)
YtrainT = sub("Fully ", "", YtrainT) %>% as.factor() %>% unlist()
YtestT = as.character(YtestRelevel)
YtestT = sub("Fully ", "", YtestT) %>% as.factor() %>% unlist()
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE))
plot(rfOut)
rfOut$bestTune
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 200,
tuneGrid = rfOut$bestTune,
metric = 'Accuracy',
trControl = trainControl(method = 'none', classProbs=TRUE))
rfImportance = rfOut$finalModel$variable.importance/sum(rfOut$finalModel$variable.importance)
rfImportance
#ROC curve
YhattestRF = predict(rfOut, XtestMat, type='prob')
rocCurveRF = roc(YtestT, YhattestRF$`Default`)
plot(rocCurveRF, legacy.axes=TRUE)
rocCurveRF$auc
#confusion matrix
YhattestRF = predict(rfOut, XtestMat)
confusionMatrixOutRF = confusionMatrix(reference = YtestT, data = YhattestRF)
confusionMatrixOutRF$table
print(confusionMatrixOutRF$overall[1:2])
print(confusionMatrixOutRF$byClass[1:2])
rfImportance
rfOut$finalModel$variable.importance/sum(rfOut$finalModel$variable.importance)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE))
plot(rfOut)
rfOut$bestTune
rfImportance = rfOut$finalModel$variable.importance/sum(rfOut$finalModel$variable.importance)
rfImportance
rfOut$finalModel$variable.importance
rfOut$finalModel
varImp(rfOut)
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 200,
tuneGrid = rfOut$bestTune,
metric = 'Accuracy',
trControl = trainControl(method = 'none', classProbs=TRUE))
#variable Importance
YhattestRF = predict(rfOut, XtestMat, type='prob')
varImp(YhattestRF)
YhattestRF
varImp(rfOut)
rfOut$variable.importance
str(rfOut)
rfOutf$variable.importance$Importance
rfOut$variable.importance$Importance
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE),
importance='impurity')
rfOut$variable.importance$Importance
plot(rfOut)
rfOut$bestTune
rfOut$variable.importance
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 200,
tuneGrid = rfOut$bestTune,
metric = 'Accuracy',
trControl = trainControl(method = 'none', classProbs=TRUE))
#ROC curve
YhattestRF = predict(rfOut, XtestMat, type='prob')
rocCurveRF = roc(YtestT, YhattestRF$`Default`)
plot(rocCurveRF, legacy.axes=TRUE)
rocCurveRF$auc
#confusion matrix
YhattestRF = predict(rfOut, XtestMat)
confusionMatrixOutRF = confusionMatrix(reference = YtestT, data = YhattestRF)
confusionMatrixOutRF$table
print(confusionMatrixOutRF$overall[1:2])
print(confusionMatrixOutRF$byClass[1:2])
rfOut$variable.importance$Importance
rfOut$variable.importance
rfOut$modelInfo
varImp(rfOut)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE),
importance=TRUE)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE),
importance=T)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5),
importance=T)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE))
varImp(rfOut)
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(15,20,25,35),
mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainT,
method = "ranger",
num.trees = 150,
tuneGrid = tuneGridRanger,
metric = 'Accuracy',
trControl = trainControl(method = "cv", number = 5,classProbs=TRUE),
importance = 'permutation')
varImp(rfOut)
