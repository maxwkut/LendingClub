---
title: "Predicting Credit Defaults"
author: "Bradley Gravitt, Max Kutschinski, Shree Karimkolikuzhiyil"
output: github_document

---
`r format(Sys.time(), '%d %B, %Y')`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packs = c('dplyr','ggplot2','caret','corrplot','e1071','readr', 'reshape2', 'pROC', 'glmnet')
lapply(packs,require,character.only=T)
```

```{r data, include=F}
data = read_csv("Data/LoanStats3a.csv")
```


# Overview

Here we filtered the loan_status column to keep only the rows where the loan was either in default/charged off or was fully paid. 

```{r echo=F}
#Transform loan status
data$loan_status[data$loan_status == "Charged Off"] = "Default"
data$loan_status[data$loan_status == "Does not meet the credit policy. Status:Charged Off"] = "Default"
data= data %>% filter(loan_status == "Default" | loan_status == "Fully Paid")

unique(data$loan_status)
data%>%select(loan_status)%>%group_by(loan_status)%>%count(.)
```

# Exploratory Analysis

## Missing Values

The goal of this section is to identify features that are eligible for feature wise deletion in order to make the data set easier to navigate. Part II discusses how to handle any remaining missing values. The original dataset contains 39719 observations and 111 variables.

```{r include=F}
anyNA(data) # to see if there are any missing values in the dataset
dim(data) # number of observations and features
```

```{r echo=F, warning=F, message=F, error=F}
ggplot_missing <- function(x){
if(!require(reshape2)){warning('you need to install reshape2')}
require(reshape2)
require(ggplot2)
#### This function produces a plot of the missing data pattern
#### in x. It is a modified version of a function in the 'neato' package
x %>%
  is.na %>%
  melt %>%
  ggplot(data = .,
         aes(x = Var2,
             y = Var1)) +
  geom_raster(aes(fill = value)) +
  scale_fill_grey(name = "",
                  labels = c("Present","Missing")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, vjust=0.5)) +
  labs(x = "Variables in Dataset",
       y = "Rows / observations")
}

```

```{r echo=F}
ggplot_missing(data) # visualize missing data
```

The missingness plot indicates that a large amount of features have a high percentage of missing values. The following output shows the exact percentages per feature.

```{r echo=F}
round(colMeans(is.na(data))*100,3)
```

Thus, it makes sense to delete features with a large percentage of missing values. The general rule of thumb is to delete any features with 33% or more.

```{r include=F}

data = data %>% select_if(~mean(is.na(.))<=0.33) # drop features with a lot of NAs
dim(data) # dimensions of altered dataset
```

After dropping the features that fall into this category, 54 variables remain in the dataset.  Before using any kind of imputation method the data is split into a training and test set. Imputation is only performed on the features of the training set. Mode imputation is used for qualitative features, and imputation using median values or linear methods are used for quantitative features.

## Data structures

The following section of code explores the data structures in order to identify any qualitative features that might be coded as quantitative features and vice versa. First, the number of features per data type is displayed.

```{r echo=F}
table(sapply(data[1,],class)) # number of features per data type
```

To get a closer look at the data types for each feature, the following output can be used. 

```{r echo=F}
str(data) # overview of data types
```

Furthermore, the number of unique values per features are displayed.

```{r echo=F}
sapply(data,function(x){ length(unique(x))})
```

There are some qualitative features, some coded as character and some as numeric, that need to be converted to factors. In addition, it seems like some features only have one value (besides NA) and should therefore be dropped. We also found that we will likely be unable to extract any useful information from some features. We also tried to keep only the features that were available to an investor before they invested on the loan. After making some transformations, the dataset looks as follows.

```{r echo=F}

# features with only one value that will be dropped:
#     collections_12_mths_ex_med, 
#     application_type, 
#     policy_code,
#     chargeoff_within_12_mths
#     init_list_status
#     acc_now_delinq
#     delinq_amnt

data = data %>% select(-c(collections_12_mths_ex_med, 
                          application_type, 
                          policy_code, 
                          chargeoff_within_12_mths, 
                          initial_list_status, 
                          acc_now_delinq, 
                          delinq_amnt, 
                          tax_liens))

# These features are uninformative and will be dropped:
# We may need to state why they are uninformative.
# zip_code 
# id 
# member_id 
# emp_title 
# title
# url
# desc

data = data %>% select(-c(zip_code, 
                          id, 
                          member_id, 
                          emp_title, 
                          title, 
                          url, 
                          desc))


# Keep only the features that are available to us before someone invested on the loan.
# Features that were not available before loan disbursement:

# total_pymnt
# total_pymnt_inv
# total_rec_prncp, 
# total_rec_int, 
# total_rec_late_fee, 
# recoveries, 
# collection_recovery_fee,
# last_pymnt_d,
# last_pymnt_amnt,  

data = data %>% select(-c(total_pymnt, 
                                  total_pymnt_inv, 
                                  total_rec_prncp, 
                                  total_rec_int, 
                                  total_rec_late_fee, 
                                  recoveries, 
                          collection_recovery_fee, 
                          last_pymnt_d, 
                          last_pymnt_amnt))


##### Dates converted to factors for now

# Are you guys cool with converting some of these dates to numeric values where we can? # I think they could be a little more useful this way instead of just leaving them out of the model or converting them all as factors.

#issue_d: date
#last_credit_pull_d: date
#earliest_cr_line: date

# Converting employment length to number of years they have been employed.
data$emp_length[which(data$emp_length == "10+ years")] = 10
data$emp_length[which(data$emp_length == "< 1 year")] = 0
data$emp_length[which(data$emp_length == "n/a")] = NA
data$emp_length = sub(" years", "", data$emp_length)
data$emp_length = sub(" year", "", data$emp_length)
data$emp_length = as.numeric(data$emp_length)

#Converted issue_d to the year that the loan was issued
data$issue_d = sub("-.*", "", data$issue_d)

#This code will convert earliest_cr_line to the number of years since their earliest credit line. Also, I think this was last updated in 2008 since the earliest credit line for someone was in 2008.
data$earliest_cr_line = sub("-[[:alpha:]]+","", data$earliest_cr_line)
data$earliest_cr_line = sub(".*-", "", data$earliest_cr_line) %>% as.numeric(.)
current = 8 - data$earliest_cr_line[data$earliest_cr_line <= 8]
old = na.omit(data$earliest_cr_line[data$earliest_cr_line > 8]) %>% as.character(.)
old = paste("19",old)
old = sub(" ","", old) %>% as.numeric(.)
old = 2008 - old
current = as.character(current)
old = as.character(old)
data$earliest_cr_line = as.numeric(c(current, old))


#This converts last_credit_pull_d to the year of their last credit pull.
data$last_credit_pull_d = sub("-.*", "", data$last_credit_pull_d)

#Gets rid of the percent signs for revol_util and int_rate and converts them to numeric
data$revol_util = as.numeric(sub("%","",data$revol_util)) 
data$int_rate = as.numeric(sub("%","",data$int_rate))


#convert data types
dataQual = data %>% select(c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          issue_d, 
                          loan_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state, 
                          last_credit_pull_d)) %>% mutate_all(factor)

dataQuan = data %>% select(-c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          issue_d, 
                          loan_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state,
                          last_credit_pull_d))

#final result
table(sapply(cbind(dataQual[1,],dataQuan[1,]),class)) # number of features per data type
str(dataQual)
str(dataQuan)
```
 

# Ensuring nontrivial variation, centering and scaling,
```{r}
#Removing features that do not have at least some nontrivial variation

sdVec = apply(dataQuan, 2, sd, na.rm=TRUE)
which(sdVec < 0.0001)

#Everything has at least some nontrivial variation

#Checking features for imbalanced frequencies. Pg 45 in the book recommends removing features if the fraction of unique values to sample size is low (10%) and the ratio of the most prevalent value to 2nd most prevalent value is large (around 20). if both criteria holds they say it may be advantageous to remove it. On pg 55 they show the code for this.

nearZeroVar(dataQuan, saveMetrics = TRUE)
table(dataQuan$out_prncp)
table(dataQuan$out_prncp_inv)

#these are all good candidates to remove based on this metric so I am going to remove them.
dataQuan = dataQuan %>% select(-c(out_prncp,
                                  out_prncp_inv))

#Centering and scaling the numeric features

dataQuan = dataQuan %>% preProcess(.) %>% predict(newdata = dataQuan)
str(dataQuan)
```


## Extreme observations and skewness

Principal components analysis is used to check for extreme observations. However, it is important to check for skewness first.
Assuming that acceptable values of skewness fall between -1,5 and 1.5, features with values for skewness outside of this range are transformed.


- I think it may be better do this before we split the data into a training and test set
- Also I dont think we need to correct for skewness since we are using logistic regression but im not sure. So maybe we assign the transformed variable used for PCA to something other than what we will use when doing the train/test split?
- I also couldnt find anything in the book about this. In the book they only seem to correct for skew when using linear regression.
- I read an article that says "in skewed data, the tail region may act as an outlier for the statistical model and we know outliers adversely affect model performance especially regression based models. Maybe we try the model with and without transformed data to see how it affects performance?
- Bradley

```{r echo=F}
(skewed= apply(dataQuan, 2, skewness, na.rm=TRUE))
```

The output indicates that some features are heavily skewed.

Here we correct for the skewness using the Yeo Johnson method since we are going to visualize outliers using PCA.
```{r echo=F}

skewFeats = names(which(abs(skewed) > 1.5))

dataQuanYJ = dataQuan %>%
  select(abs(contains(skewFeats))) %>%
  preProcess(method = 'YeoJohnson', na.rm=TRUE) %>%
  predict(dataQuan %>% select(contains(skewFeats)))

dataQuanNotSkew = dataQuan %>% select(!contains(skewFeats))

apply(dataQuanYJ, 2, skewness, na.rm=TRUE)

dataQuanYJ = cbind(dataQuanYJ, dataQuanNotSkew)
```

The transformation only had an effect on annual_inc, inq_last_6mnths, and revol_bal.


Extreme observations can be identified via the following PCA output.

```{r echo=F}
pcaOut = prcomp(na.omit(dataQuanYJ,scale=TRUE,center=TRUE))
XQuanScores = data.frame(pcaOut$x)
ggplot(data = XQuanScores) + geom_point(aes(x = PC1, y = PC2)) +
  coord_cartesian(xlim=range(XQuanScores$PC1), ylim=range(XQuanScores$PC2))

```

Okay, so there are not really any outliers. But if we keep out_prncp, out_prncp_inv, pub_rec_bankruptcies then there were 4 extreme observations apparent in PC2.

This is the code to check out those extreme observations that are only there if out_prncp, out_prncp_inv, pub_rec_bankruptcies are put back into the dataQuan
```{r}
(extremeObs = which(XQuanScores$PC2 < -10))

str(data[extremeObs, ])
```

When I had done this with out_prncp, out_prncp_inv, pub_rec_bankruptcies in dataQuan what really stood out to me was the annual income and revolving balances the outliers had. That is probably why they were outliers. It looked like everything was entered properly so if we decide to keep those features then I dont think we have a reason to delete them? Maybe we should remove them purely for the sake of model performance and since we want the model to be as applicable as possible?

This would probably be a good question to ask, if we keep out_prncp, out_prncp_inv, pub_rec_bankruptcies.

#Missing Data

we know that there will be no missing data for the qualitative features since we will impute the mode for them. No imputation for the test set we will just predict them with the model.

```{r}

#Counting the missing data
sapply(dataQuan, function(x) sum(is.na(x))) 

```
There are not really a lot of missing values for most of our features. Since emp_length isnt really correlated with anything (you can see in the corrplot later), we wont be able to use a linear model to impute values for it. I think it is safe to just impute the median value for these. 

#Splitting data into training and test set and also handling the class imbalance

I am going to downsample to readjust the class frequencies for loan_status since there are so many more cases of fully paid than default. I think what happens if you dont is that the model gets really good at predicting fully paid and then predicts almost everything as fully paid. You can check page 439 of the book where it has the code for this. pg 427 last paragraph discusses the theory behind this. When tested this I saw an improvement in accuracy.

```{r IMPUTATION, include=T}
set.seed(123)

Y = select(dataQual,loan_status) %>% unlist()

# NOTE: Might want to consider stratified train test split because of unequal proportions in the supervisor

trainIndex = createDataPartition(Y, p=.75, list= FALSE) %>% as.vector()
Y = as.data.frame(Y)

#code for downsampling
XFullTrain = cbind(dataQual, dataQuan)
XFullTrain = XFullTrain[trainIndex, ]

predictors = select(XFullTrain, -loan_status)
outcome = select(XFullTrain, loan_status) %>% unlist(.)
trainingData = downSample(x=predictors, y=outcome, yname='loan_status')



# split train features into qual and quan for imputation
XQualtrain = trainingData %>% select(c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          issue_d, 
                          pymnt_plan, 
                          purpose, 
                          addr_state, 
                          last_credit_pull_d))

XQuantrain = trainingData %>% select(-c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          issue_d, 
                          loan_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state,
                          last_credit_pull_d))

Ytrain     = trainingData %>% select(loan_status) %>% unlist()

#You can see how downsampling balances out the class imbalance
table(Y)
table(Ytrain)
33314/6405

#combine qual. and quan. features 
Xtest      = select(cbind(dataQual[-trainIndex,],dataQuan[-trainIndex,]), -loan_status)
Ytest      = Y[-trainIndex,]

modeImpute = function(Xqual){
  tbl = table(Xqual)
  Xqual[is.na(Xqual)] = names(tbl)[which.max(tbl)]
  return(Xqual)
}

XQuantrain = XQuantrain %>% 
  preProcess(method = 'medianImpute')%>%
  predict(newdata = XQuantrain)

XQuantest = XQuantest %>% 
  preProcess(method = 'medianImpute')%>%
  predict(newdata = XQuantrain)

XQualtrain = XQualtrain %>% mutate(across(.cols=everything(), modeImpute))

#Check missing one more time to be sure the code worked (No more missing values!)
sapply(XQuantrain, function(x) sum(is.na(x)))

#Here are the structures if you guys are interested
str(XQualtrain)
str(XQuantrain)
str(Ytrain)
str(Xtest)
str(Ytest)
```


## Removing correlated variables

Quantitative features with high correlation (p>0.85) are problematic and should removed as a result.

```{r, warning = F, error = F, message = F, echo=F}
datacorr = cor(XQuantrain)
corrplot(datacorr, order= 'hclust', t1.cex= .35)
str(XQuantrain)
dim(XQuantrain)
```

```{r include=T}
highCorr = findCorrelation(datacorr, .85, verbose=T, names=T)
XQuantrain= select(all_of(XQuantrain), -any_of((highCorr)))
Xtest= select(all_of(Xtest), -any_of((highCorr)))
str(XQuantrain)
dim(XQuantrain)
```

The correlation plot indicates that there are a few features with high correlation in the dataset that should be removed. 


Here is our final list of features after preprocessing.
```{r echo=F}
Xtrain = cbind(XQualtrain, XQuantrain) # combine qualitative and quantitative features
dim(Xtrain) # total number of observations and features in the training set
names(Xtrain) # names of remaining features
```

#Dummy variable coercion and creating full feature matrices

```{r}

XQualtest = select_if(Xtest, is.factor)

dummyModel = dummyVars(~., data = XQualtrain, fullRank=TRUE)

XQualtrainDummy = predict(dummyModel, XQualtrain)
XtrainFull = cbind(XQualtrainDummy, XQuantrain)

XQualtestDummy = predict(dummyModel, XQualtest)
XQuantest = select_if(Xtest, is.numeric)
XtestFull = cbind(XQualtestDummy, XQuantest)

dim(XtrainFull)

```



# Methods


#Logistic Regression

```{r}
str(Ytrain)
#code for the logistic regression model
YtrainRelevel = relevel(Ytrain, ref = 'Default')
YtestRelevel = relevel(Ytest, ref = 'Default')
trControl = trainControl(method = 'none')

outLogistic = train(x = XtrainFull, y = YtrainRelevel,
                    method = 'glm', trControl = trControl)
summary(outLogistic)

YhatTestProb = predict(outLogistic, XtestFull, type = 'prob')

#Here are the probabilities
head(YhatTestProb)

```

I think the "prediction from rank-deficient fit" may be coming from some of the sub_grades having NA values for their coefficients.

Should we try removing variables to maximize AIC? This may be a good question to ask.

```{r}
#Checking how well calibrated the probabilities are

calibProbs = calibration(YtestRelevel ~ YhatTestProb$`Default`)
xyplot(calibProbs)

```



```{r}

#Getting default confusion matrix
YhatTest = predict(outLogistic, XtestFull, type = 'raw')
confusionMatrixOut = confusionMatrix(reference = YtestRelevel, data = YhatTest)
print(confusionMatrixOut$table)
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```
``` {r}

#ROC curve
rocCurve = roc(Ytest, YhatTestProb$`Default`)
plot(rocCurve, legacy.axes=TRUE)
rocCurve$auc

thresholds = rocCurve$thresholds
sort(thresholds)[1:3]

sort(thresholds, decreasing = TRUE)[1:3]

```


```{r}
#Getting confusion matrix for particular sensitivity (I dont think we need to do this)

pt5 = which.min(rocCurve$sensitivities >= 0.8) 
threshold = thresholds[pt5]
specificity = rocCurve$specificities[pt5]
sensitivity = rocCurve$sensitivities[pt5]

YhatTestThresh = ifelse(YhatTestProb$`Default` > threshold,
                        'Default', 'Fully Paid') %>% as.factor()

confusionMatrixOut = confusionMatrix(reference = YtestRelevel, data = YhatTestThresh)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```

## Logistic elastic net

```{r echo=T}
# Xtrain, Xtest, Ytrain, Ytest

set.seed(2)

XtrainMat = as.matrix(XtrainFull)
XtestMat = as.matrix(XtestFull)

#fitting the logistic elastic net 
K = 3
#I set k=3 for now so that way it doesnt take forever to run.

#Need to increase the range that we allow lamda take on.
trainControl = trainControl(method = "cv", number = K)
tuneGrid = expand.grid('alpha'=c(0,.25,.5,.75, 1), 
                       'lambda' = seq(0.0001, .001, length.out = 30))

elasticOut = train(x = XtrainMat, y = Ytrain, 
                   method = "glmnet", 
                   trControl = trainControl, 
                   tuneGrid = tuneGrid)

elasticOut$bestTune

#Getting the fitted model using the CV minimizing solution
glmnetOut = glmnet(x = XtrainMat, y = relevel(Ytrain, ref = 'Default'), 
                   alpha = elasticOut$bestTune$alpha, family = 'binomial', 
                   standardize = FALSE)

probHatTestGlmnet = predict(glmnetOut, XtestMat, s=elasticOut$bestTune$lambda, 
                            type = 'response')
YhatTestGlmnet = ifelse(probHatTestGlmnet > 0.5, 'Fully Paid', 'Default')

#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat
Sglmnet = abs(betaHat[-1]) > .005


#Confusion Matrix
confusionMatrixEL = table(YhatTestGlmnet, Ytest)
confusionMatrixEL


#ROC curve
rocOut = roc(response=Ytest, probHatTestGlmnet)
plot(rocOut)
rocOut$auc

```

The logistic elastic net regression seemed to perform better than the logistic regression. I was a little unsure about settings for alpha and lamda in the tuneGrid. Maybe we could ask how we could optimize these settings?

## SVM

```{r}

```


# Results

# Conclusion


```{r include=F}
#NOTES
# K            = 5
# trainControl = trainControl(method = "cv", number = K)
# tuneGrid     = expand.grid('alpha'=c(.5, 1),'lambda' = seq(0.0001, .01, length.out = 10))
# 
# elasticOut   = train(x= Xtrain, y= Ytrain,
#                      method = "glmnet",
#                      trControl = trainControl, 
#                      tuneGrid = tuneGrid) #### Answer 1.1
# 
# elasticOut$bestTune
# 
# glmnetOut         = glmnet(x = XtrainMat, y = relevel(Ytrain, ref = 'X8'), 
#                            alpha = elasticOut$bestTune$alpha, family = 'binomial')
# probHatTestGlmnet = predict(glmnetOut, XtestMat, s=elasticOut$bestTune$lambda, type = 'response')
# YhatTestGlmnet    = ifelse(probHatTestGlmnet>0.5, 'X1', 'X2')# check with roc curve

```
