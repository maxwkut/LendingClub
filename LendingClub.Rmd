---
title: "Predicting Credit Defaults"
author: "Bradley Gravitt, Max Kutschinski, Shree Karimkolikuzhiyil"
output: github_document

---
`r format(Sys.time(), '%d %B, %Y')`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packs = c('dplyr','ggplot2','caret','corrplot','e1071','readr', 'reshape2', 'pROC', 'glmnet', 'keras')
lapply(packs,require,character.only=T)
```

```{r data, include=F}
data = read_csv("Data/LoanStats3a.csv")
```


# Overview

Here we filtered the loan_status column to keep only the rows where the loan was either in default/charged off or was fully paid. 

```{r echo=F}
#Transform loan status
data$loan_status[data$loan_status == "Charged Off"] = "Default"
data$loan_status[data$loan_status == "Does not meet the credit policy. Status:Charged Off"] = "Default"
data= data %>% filter(loan_status == "Default" | loan_status == "Fully Paid")

unique(data$loan_status)
data%>%select(loan_status)%>%group_by(loan_status)%>%count(.)
```


So, from the comments the professor made on our task 3, I am gathering that we could first split the data into X and Y, do all the preprocessing steps, then do the training test split?

```{r}
x = data %>% select(-loan_status)
y = data %>% select(loan_status)

```

# Exploratory Analysis


## Missing Values

The goal of this section is to identify features that are eligible for feature wise deletion in order to make the data set easier to navigate. Part II discusses how to handle any remaining missing values. The original dataset contains 39719 observations and 111 variables.

```{r include=T}
anyNA(x) # to see if there are any missing values in the training set
dim(x) # number of observations and features in the training set
```


```{r echo=F, warning=F, message=F, error=F}
ggplot_missing <- function(x){
if(!require(reshape2)){warning('you need to install reshape2')}
require(reshape2)
require(ggplot2)
#### This function produces a plot of the missing data pattern
#### in x. It is a modified version of a function in the 'neato' package
x %>%
  is.na %>%
  melt %>%
  ggplot(data = .,
         aes(x = Var2,
             y = Var1)) +
  geom_raster(aes(fill = value)) +
  scale_fill_grey(name = "",
                  labels = c("Present","Missing")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, vjust=0.5)) +
  labs(x = "Variables in Dataset",
       y = "Rows / observations")
}

```

```{r echo=F}
ggplot_missing(x) # visualize missing data
```

The missingness plot indicates that a large amount of features have a high percentage of missing values. The following output shows the exact percentages per feature.

```{r echo=F}
round(colMeans(is.na(x))*100,3)
```

Thus, it makes sense to delete features with a large percentage of missing values. The general rule of thumb is to delete any features with 33% or more.

```{r}
# drop features with a lot of NAs from train and test set
x = x %>% select_if(~mean(is.na(.))<=0.33)

# dimensions of altered dataset
dim(x)
```

After dropping the features that fall into this category, 53 variables remain in the train and test set.  

!!!!!Before using any kind of imputation method the data is split into a training and test set. Imputation is only performed on the features of the training set. Mode imputation is used for qualitative features, and imputation using median values or linear methods are used for quantitative feature!!!

## Data structures

The following section of code explores the data structures in order to identify any qualitative features that might be coded as quantitative features and vice versa. First, the number of features per data type is displayed.

```{r echo=F}
table(sapply(x[1,],class)) # number of features per data type
```

To get a closer look at the data types for each feature, the following output can be used. 

```{r echo=F}
str(x) # overview of data types
```

Furthermore, the number of unique values per features are displayed.

```{r echo=F}
sapply(x,function(x){ length(unique(x))})
```

There are some qualitative features, some coded as character and some as numeric, that need to be converted to factors. In addition, it seems like some features only have one value (besides NA) and should therefore be dropped. We also found that we will likely be unable to extract any useful information from some features. We also tried to keep only the features that were available to an investor before they invested on the loan. After making some transformations, the dataset looks as follows.

```{r echo=F}

# features with only one value that will be dropped:
#     collections_12_mths_ex_med, 
#     application_type, 
#     policy_code,
#     chargeoff_within_12_mths
#     init_list_status
#     acc_now_delinq
#     delinq_amnt

x = x %>% select(-c(collections_12_mths_ex_med, 
                          application_type, 
                          policy_code, 
                          chargeoff_within_12_mths, 
                          initial_list_status, 
                          acc_now_delinq, 
                          delinq_amnt, 
                          tax_liens))

# These features are uninformative and will be dropped:
# We may need to state why they are uninformative.
# zip_code 
# id 
# member_id 
# emp_title 
# title
# url
# desc

x = x %>% select(-c(zip_code, 
                          id, 
                          member_id, 
                          emp_title, 
                          title, 
                          url, 
                          desc))


# Keep only the features that are available to us before someone invested on the loan.
# Features that were not available before loan disbursement:

# total_pymnt
# total_pymnt_inv
# total_rec_prncp, 
# total_rec_int, 
# total_rec_late_fee, 
# recoveries, 
# collection_recovery_fee,
# last_pymnt_d,
# last_pymnt_amnt,  

x = x %>% select(-c(issue_d, 
                    total_pymnt, 
                                  total_pymnt_inv, 
                                  total_rec_prncp, 
                                  total_rec_int, 
                                  total_rec_late_fee, 
                                  recoveries, 
                          collection_recovery_fee, 
                          last_pymnt_d, 
                          last_pymnt_amnt))


##### Dates converted to factors for now

# Are you guys cool with converting some of these dates to numeric values where we can? # I think they could be a little more useful this way instead of just leaving them out of the model or converting them all as factors.

#issue_d: date
#last_credit_pull_d: date
#earliest_cr_line: date

# Converting employment length to number of years they have been employed.
x$emp_length[which(x$emp_length == "10+ years")] = 10
x$emp_length[which(x$emp_length == "< 1 year")] = 0
x$emp_length[which(x$emp_length == "n/a")] = NA
x$emp_length = sub(" years", "", x$emp_length)
x$emp_length = sub(" year", "", x$emp_length)
x$emp_length = as.numeric(x$emp_length)

#*** professor said we should remove this **#
#Converted issue_d to the year that the loan was issued
#x$issue_d = sub("-.*", "", x$issue_d)
#x$issue_d = sub("-.*", "", x$issue_d)
#x$issue_d = sub("-.*", "", x$issue_d)

#This code will convert earliest_cr_line to the number of years since their earliest credit line.
x$earliest_cr_line = sub("-[[:alpha:]]+","", x$earliest_cr_line)
x$earliest_cr_line = sub(".*-", "", x$earliest_cr_line) %>% as.numeric(.)
x$earliest_cr_line = ifelse(x$earliest_cr_line <= 8, 8-x$earliest_cr_line, x$earliest_cr_line)
x$earliest_cr_line = as.character(x$earliest_cr_line)
x$earliest_cr_line = sub(" ", "", x$earliest_cr_line) %>% as.numeric(.)
x$earliest_cr_line = ifelse(x$earliest_cr_line > 8, paste(19,x$earliest_cr_line), x$earliest_cr_line)
x$earliest_cr_line = sub(" ", "", x$earliest_cr_line)
x$earliest_cr_line = as.numeric(x$earliest_cr_line)
x$earliest_cr_line = ifelse(x$earliest_cr_line > 8, 2008 - x$earliest_cr_line, x$earliest_cr_line)


#This converts last_credit_pull_d to the year of their last credit pull.
x$last_credit_pull_d = sub("-.*", "", x$last_credit_pull_d)

#Gets rid of the percent signs for revol_util and int_rate and converts them to numeric
x$revol_util = as.numeric(sub("%","",x$revol_util))

x$int_rate = as.numeric(sub("%","",x$int_rate))


#convert data types

#training data
xQual = x %>% select(c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state, 
                          last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)

xQuan = x %>% select(-c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state,
                          last_credit_pull_d)) %>% as.data.frame(.)

#final result
table(sapply(cbind(xQual[1,],xQuan[1,]),class)) # number of features per data type
str(xQual)
str(xQuan)
str(y)
```
 
## Extreme observations and skewness

Principal components analysis is used to check for extreme observations. However, it is important to check for skewness first.
Assuming that acceptable values of skewness fall between -1,5 and 1.5, features with values for skewness outside of this range are transformed.
```{r echo=F}
print("train")
(skewed= apply(xQuan, 2, skewness, na.rm=TRUE))
```

The output indicates that some features are heavily skewed.

Here we correct for skewness using the Yeo Johnson method since we are going to visualize outliers using PCA.
```{r echo=F}

#center and scale
xQuanPCA = xQuan %>% preProcess(.) %>% predict(newdata = xQuan)

#Correcting for skewness
skewFeats = names(which(abs(skewed) > 1.5))

xQuanYJ = xQuanPCA %>%
  select(abs(contains(skewFeats))) %>%
  preProcess(method = 'YeoJohnson', na.rm=TRUE) %>%
  predict(xQuanPCA %>% select(contains(skewFeats)))

xQuanNotSkew = xQuanPCA %>% select(!contains(skewFeats))

apply(xQuanYJ, 2, skewness, na.rm=TRUE)

xQuanYJ = cbind(xQuanYJ, xQuanNotSkew)
```

The transformation only had an effect on annual_inc, inq_last_6mnths, and revol_bal.


Extreme observations can be identified via the following PCA output.
```{r echo=F}
pcaOut = prcomp(na.omit(xQuanYJ,scale=TRUE,center=TRUE))
xQuanScores = data.frame(pcaOut$x)
ggplot(data = xQuanScores) + geom_point(aes(x = PC1, y = PC2)) +
  coord_cartesian(xlim=range(xQuanScores$PC1), ylim=range(xQuanScores$PC2))

```

It looks like there are 4 outliers. I had to center and sale the data in order to see the 4 extreme observations.

This is the code to check out those extreme observations.
```{r}
(extremeObs = which(xQuanScores$PC2 < -10))

str(xQuan[extremeObs, ])

```

What really stood out to me was the annual income and revolving balances the outliers had. That is probably why they were outliers. It looked like everything was entered properly so I dont think we have a reason to delete them? Maybe we should remove them purely for the sake of model performance and since we want the model to be as applicable as possible?

#Missing Data

we know that there will be no missing data for the qualitative features since we will impute the mode for them. No imputation for the test set we will just predict them with the model.

```{r}

#Counting the missing data
sapply(xQuan, function(x) sum(is.na(x))) 

```

There are not really a lot of missing values for most of our features. Since emp_length isnt really correlated with anything (you can see in the corrplot later), we wont be able to use a linear model to impute values for it. I think it is safe to just impute the median value for these.

```{r}
#mode impute
modeImpute = function(Xqual){
  tbl = table(Xqual)
  Xqual[is.na(Xqual)] = names(tbl)[which.max(tbl)]
  return(Xqual)
}

xQual = xQual %>% mutate(across(.cols=everything(), modeImpute))

#median impute
xQuan = xQuan %>% 
  preProcess(method = 'medianImpute')%>%
  predict(newdata = xQuan)

#Counting the missing data again
sapply(xQual, function(x) sum(is.na(x))) 
sapply(xQuan, function(x) sum(is.na(x)))

```

## Removing correlated variables

Quantitative features with high correlation (p>0.85) are problematic so we will remove them here.

```{r, warning = F, error = F, message = F, echo=F}
datacorr = cor(xQuan)
corrplot(datacorr, order= 'hclust', t1.cex= .35)
str(xQuan)
dim(xQuan)
```

```{r include=T}
#correlated features
highCorr = findCorrelation(datacorr, .85, verbose=T, names=T)
highCorr
```


So here the first chunk removes the correlated variables and the second chunk uses PCA on them to keep some information from them. We can choose either one.

```{r}
#Removing correlated features
xQuan= select(all_of(xQuan), -any_of((highCorr)))
```

```{r}
#PCA on correlated features
corrFeats = select(xQuan, c(funded_amnt, loan_amnt, funded_amnt_inv))
corrFeatsPCA = preProcess(corrFeats, method = 'pca', thresh = 0.90) %>% predict(corrFeats)
#Putting features back into dataframe as PCA
xQuan = cbind(corrFeatsPCA, xQuan)
#Removing correlated features
xQuan= select(all_of(xQuan), -any_of((highCorr)))
#Checking correlation after PCA
datacorr = cor(xQuan)
corrplot(datacorr, order= 'hclust', t1.cex= .35)
highCorr = findCorrelation(datacorr, .85, verbose=T, names=T)
highCorr

str(xQuan)

```

The correlation plot indicates that there are a few features with high correlation in the dataset that were removed. 

# Centering and scaling and ensuring nontrivial variation
```{r}


#Centering and scaling the numeric features (dont want to center and scale dummy variables)
xQuan = xQuan %>% preProcess(.) %>% predict(newdata = xQuan)

#Checking features for imbalanced frequencies. Pg 45 in the book recommends removing features if the fraction of unique values to sample size is low (10%) and the ratio of the most prevalent value to 2nd most prevalent value is large (around 20). if both criteria holds they say it may be advantageous to remove it. On pg 55 they show the code for this.

#I asked about this is the Q&A and he pretty much said either method is fine to use. I am going to try it with and without it.

#str(xQuan)
#freq = nearZeroVar(xQuan, saveMetrics = TRUE)
#xQuan = select_if(xQuan, freq$freqRatio < 30 | freq$percentUnique > 0.10)
#str(xQuan)

```


#Here is our final list of features after preprocessing.
```{r echo=F}
xFull = cbind(xQual, xQuan) # combine qualitative and quantitative features
dim(xFull)# total number of observations and features in the training set
names(xFull)# names of remaining features
```

#Splitting data into training and test set
```{r IMPUTATION, include=T}
set.seed(1234567)

#For downsampling, need to make data a data frame and loan_status a factor for the split. I dont think I need to do it if i dont.
#data = as.data.frame(data)
#data$loan_status = as.factor(data$loan_status)

xFull = cbind(y, xFull) %>% as.data.frame()

n=nrow(xFull)
trainingDataIndex = createDataPartition(xFull$loan_status, p = .75, list = FALSE) %>% as.vector(.)
trainingData = xFull[trainingDataIndex, ]
testingData = xFull[-trainingDataIndex, ]

Xtrain = select(trainingData, -loan_status)
Xtest = select(testingData, -loan_status)
Ytrain = select(trainingData, loan_status) %>% unlist() %>% as.factor()
Ytest = select(testingData, loan_status) %>% unlist() %>% as.factor()

rm(trainingData)
rm(testingData)

#splitting the sets into qualitative and quantitative for dummy variable coercion 

#training data
XtrainQual = Xtrain %>% select(c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state, 
                          last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)

XtrainQuan = Xtrain %>% select(-c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state,
                          last_credit_pull_d)) %>% as.data.frame(.)

#test data
XtestQual = Xtest %>% select(c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state, 
                          last_credit_pull_d)) %>% mutate_all(factor) %>% as.data.frame(.)

XtestQuan = Xtest %>% select(-c(term, 
                             grade, 
                          sub_grade, 
                          home_ownership, 
                          verification_status, 
                          pymnt_plan, 
                          purpose, 
                          addr_state,
                          last_credit_pull_d)) %>% as.data.frame(.)

```

#Checking that qualitative variables have some variation
```{r}
str(XtrainQual)
str(XtestQual)
#pymnt_plan only has 1 level the test set so I am going to remove it since I cannot create dummy variables for it.

XtrainQual = XtrainQual %>% select(-pymnt_plan)
XtestQual = XtestQual %>% select(-pymnt_plan)

```

#Dummy variable coercion and creating full feature matrices
```{r}

#I am filtering for zero variance before I do this because I am getting an error that I can only create dummy variables on factors with 2 or more levels. 

#just a note for myself. I tried to create a dummy model for each set but it did not work since the sets have factors with different levels.
dummyModel = dummyVars(~., data = XtrainQual, fullRank=TRUE)

XtrainQualDummy = predict(dummyModel, XtrainQual)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)

XtestQualDummy = predict(dummyModel, XtestQual)
XtestFull = cbind(XtestQualDummy, XtestQuan)

dim(XtrainFull)
dim(XtestFull)

```


#downsampling to balance the class imbalance
```{r}

#predictors = select(trainingData, -loan_status)
#outcome = select(trainingData, loan_status) %>% unlist(.)
#trainingData = downSample(x=predictors, y=outcome, yname='loan_status')

XandYtrain = cbind(XtrainFull, Ytrain)
XandYtrain = rename(XandYtrain, loan_status=Ytrain)
predictors = select(XandYtrain, -loan_status)
outcome = select(XandYtrain, loan_status) %>% unlist(.)
XandYtrain = downSample(x=predictors, y=outcome, yname='loan_status')

XtrainFull = select(XandYtrain, -loan_status)
Ytrain = select(XandYtrain, loan_status) %>% unlist(.)

```


#Linear Methods

Ensuring correct levels for Y
```{r}
YtrainRelevel = relevel(Ytrain, ref = 'Default')
YtestRelevel = relevel(Ytest, ref = 'Default')
```

Logistic Regression
```{r}
#code for the logistic regression model
outLogistic = train(x = XtrainFull, y = YtrainRelevel,
                    method = 'glm', trControl = trainControl(method='cv',number=20))
summary(outLogistic)
YhattestProb = predict(outLogistic, XtestFull, type = 'prob')

#Checking how well calibrated the probabilities are
calibProbs = calibration(YtestRelevel ~ YhattestProb$`Default`)
xyplot(calibProbs)

#Getting default confusion matrix
Yhattest = predict(outLogistic, XtestFull, type = 'raw')
confusionMatrixOut = confusionMatrix(reference = YtestRelevel, data = Yhattest)
print(confusionMatrixOut$table)
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

#ROC curve
rocCurve = roc(Ytest, YhattestProb$`Default`)
plot(rocCurve, legacy.axes=TRUE)
rocCurve$auc

thresholds = rocCurve$thresholds
sort(thresholds)[1:3]

sort(thresholds, decreasing = TRUE)[1:3]

```
I think the "prediction from rank-deficient fit" may be coming from some of the sub_grades having NA values for their coefficients.

Should we try removing variables to maximize AIC? This may be a good question to ask.

```{r}
#Getting confusion matrix for particular sensitivity (I dont think we need to do this)

pt5 = which.min(rocCurve$sensitivities >= 0.8) 
threshold = thresholds[pt5]
specificity = rocCurve$specificities[pt5]
sensitivity = rocCurve$sensitivities[pt5]

YhattestThresh = ifelse(YhattestProb$`Default` > threshold,
                        'Default', 'Fully Paid') %>% as.factor()

confusionMatrixOut = confusionMatrix(reference = YtestRelevel, data = YhattestThresh)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```

Creating matrices for train and test sets
```{r}
XtrainMat = as.matrix(XtrainFull)
XtestMat = as.matrix(XtestFull)
```

Setting trControl. Each method will use repeated 10 fold cross validation
```{r}
K = 2
#for testing
trainControl = trainControl(method = "cv", number = K)

#this is the real one
#trainControl = trainControl(method = "repeatedcv", repeats=2, number = K)

```

Ridge Regression
```{r}
set.seed(123)
ridgeOut = cv.glmnet(XtrainMat ,YtrainRelevel, alpha=0, family='binomial', nfolds=10, standardize = FALSE)
minLambda = min(ridgeOut$lambda)
lambdaNew = seq(minLambda, minLambda*0.01,length=25)
ridgeOut = cv.glmnet(x = XtrainMat, y = YtrainRelevel, alpha = 0, family='binomial', nfolds=10, 
                     standardize=FALSE, lambda = lambdaNew)

Probridge = predict(ridgeOut, XtestMat, s = ridgeOut$lambda.min, type='response')
Yhattestridge = ifelse(Probridge > 0.5, 'Fully Paid', 'Default')

#Getting default confusion matrix
#Confusion Matrix
confusionMatrixR = table(Yhattestridge, Ytest)
confusionMatrixR

```


Lasso Regression
```{r}
set.seed(123)
lassoOut = cv.glmnet(XtrainMat ,YtrainRelevel, alpha=1, family='binomial', nfolds=10, standardize = FALSE)
minLambda = min(lassoOut$lambda)
lambdaNew = seq(minLambda, minLambda*0.01,length=25)
lassoOut = cv.glmnet(x = XtrainMat, y = YtrainRelevel, alpha = 1, family='binomial', nfolds=10, 
                     standardize=FALSE, lambda = lambdaNew)

Problasso = predict(lassoOut, XtestMat, s = lassoOut$lambda.min, type='response')
Yhattestlasso = ifelse(Problasso > 0.5, 'Fully Paid', 'Default')

#Confusion Matrix
confusionMatrixL = table(Yhattestlasso, Ytest)
confusionMatrixL

```


Logistic elastic net
```{r echo=T}
set.seed(2)

#Need to increase the range that we allow lamda take on.
tuneGrid = expand.grid('alpha'=c(0,.25,.5,.75, 1), 
                       'lambda' = seq(0.0001, .01, length.out = 100))

elasticOut = train(x = XtrainMat, y = YtrainRelevel, 
                   method = "glmnet", 
                   trControl = trainControl, 
                   tuneGrid = tuneGrid)

elasticOut$bestTune

#Getting the fitted model using the CV minimizing solution
glmnetOut = glmnet(x = XtrainMat, y = YtrainRelevel, 
                   alpha = elasticOut$bestTune$alpha, family = 'binomial', 
                   standardize = FALSE)

probHattestGlmnet = predict(glmnetOut, XtestMat, s=elasticOut$bestTune$lambda, 
                            type = 'response')
YhattestGlmnet = ifelse(probHattestGlmnet > 0.5, 'Fully Paid', 'Default')

#Active set
betaHat = coef(glmnetOut, s=elasticOut$bestTune$lambda)
betaHat = (betaHat[-1,])
important = abs(betaHat) > 0.01
betaHat[important]


#Confusion Matrix
confusionMatrixEL = table(YhattestGlmnet, Ytest)
confusionMatrixEL

#ROC curve
rocOut = roc(response=Ytest, probHattestGlmnet)
plot(rocOut)
rocOut$auc
```

The logistic elastic net regression seemed to perform better than the logistic regression. I was a little unsure about settings for alpha and lamda in the tuneGrid. Maybe we could ask how we could optimize these settings?



#Nonlinear Methods

Neural Networks
```{r}
#adjusting data types
#Xtrain
Xtrainfull = cbind(YtrainRelevel,XtrainMat) %>% as.data.frame()
XtrainfullMat = select(Xtrainfull, -YtrainRelevel) %>% as.matrix()
dimnames(XtrainfullMat) = NULL
#Ytrain
Ytrainfull = select(Xtrainfull, YtrainRelevel) %>% unlist() %>% as.vector()
Ytrainfull = to_categorical(Ytrainfull)

#Xtest
Xtestfull = cbind(YtestRelevel,XtestMat) %>% as.data.frame()
XtestfullMat = select(Xtestfull, -YtestRelevel) %>% as.matrix()
dimnames(XtestfullMat) = NULL
#Ytest
Ytestfull = select(Xtestfull, YtestRelevel) %>% unlist() %>% as.vector()
Ytestfull = to_categorical(Ytestfull)

#actual model
set.seed(123)
model = keras_model_sequential() %>% 
      layer_dense(units = 500, activation = "relu", 
                input_shape = dim(XtrainfullMat)[[2]]) %>%
      layer_dense(units = 250, activation = "relu") %>% 
                layer_dropout(rate=0.5) %>%
      layer_dense(units = 125, activation = "relu") %>% 
                layer_dropout(rate=0.5) %>%
    layer_dense(units = 63, activation = "relu") %>% 
  layer_dropout(rate=0.5) %>% 
      layer_dense(units = 31, activation = "relu") %>% 
  layer_dropout(rate=0.5) %>% 
        layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate=0.5) %>% 
  layer_dense(units = 8, activation = "relu") %>% 
  layer_dropout(rate=0.5) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dropout(rate=0.5) %>%
  layer_dense(units = 3, activation="sigmoid")

model

model = model %>% compile(optimizer = "rmsprop", 
                      loss = "binary_crossentropy", 
                      metrics = "accuracy")

my_model = model %>% fit(XtrainfullMat, Ytrainfull, epoch=15, batch_size=10, validation_data=list(XtestfullMat, Ytestfull))

model %>% evaluate(XtestfullMat, Ytestfull)

#confusion Matrix
prob = model %>% predict(XtestfullMat) %>% k_argmax() %>% as.numeric()
Ytestfull = Ytestfull %>% k_argmax() %>% as.numeric()

table(prob, Ytestfull)




```

FDA (Flexible Discriminate Analysis)
```{r}
set.seed(123)

#It says that 'Fully Paid' needs to be a test r name for it to work so I rename it to 'Paid' here
YtrainFDA = as.character(YtrainRelevel)
YtrainFDA = sub("Fully ", "", YtrainFDA) %>% as.factor() %>% unlist()
YtestFDA = as.character(YtestRelevel)
YtestFDA = sub("Fully ", "", YtestFDA) %>% as.factor() %>% unlist()

require(doParallel)
cl = makeCluster(5)
registerDoParallel(cl)
fdaOut = train(x=XtrainMat, y=YtrainFDA,
                method = 'fda', 
               metric = 'Accuracy', 
               tuneGrid = expand.grid(degree = 1:2, nprune=c(10,15,20,25,30)), 
                trControl = trainControl(method='cv', number = 3, classProbs = TRUE))
stopCluster(cl)
registerDoSEQ()

plot(fdaOut)
fdaOut$bestTune

YhattestFDA = predict(fdaOut, XtestMat, type='raw')

#Confusion Matrix
confusionMatrixOut = confusionMatrix(reference = YtestFDA, data = YhattestFDA)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```
I think you can get a measure of variable importance from this using vip fuction

SVM (Support Vector Machines)
```{r}
set.seed(123)
svmOut = train(x=XtrainMat, y=YtrainRelevel,
              method='svmLinear',
              trControl=trainControl,
              tuneGrid=expand.grid(C=seq(0.01,0.15,length.out=10)))
svmOut
plot(svmOut)

Yhattestsvm = predict(svmOut, XtestMat)

#confusion matrix
confusionMatrixOut = confusionMatrix(reference = Ytest, data = Yhattestsvm)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```

KNN
```{r}
set.seed(123)
knnOut   = train(x = XtrainMat, y = Ytrain,
                 method = "knn",
                 tuneGrid = expand.grid(k=c(100,105,110,115,120)),
                 trControl = trainControl)
knnOut$bestTune

probKNN = predict(knnOut, XtestMat, type='raw')

#confusion matrix
confusionMatrixOut = confusionMatrix(reference = Ytest, data = YhattestKNN)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])
```
I need to tune K a little more but I think it is close.

Naive Bayes
```{r}


```



#Classification Trees, Random Forest, Boosting

Classification Tree
```{r}
set.seed(123)
tuneGrid = data.frame(cp=c(0,0.005,0.008,0.01,0.013,0.015,0.02))

rpartOut = train(x = XtrainMat, y = YtrainRelevel,
                 method = "rpart",
                 tuneGrid = tuneGrid,
                 metric='Accuracy',
                 trControl = trainControl)

rpartOut$bestTune
plot(rpartOut)

plot(rpartOut$finalModel,margin= rep(.1,4))
text(rpartOut$finalModel, cex = 0.7, digits = 1)

YhattestTree = predict(rpartOut, XtestMat)

#confusion matrix
confusionMatrixOut = confusionMatrix(reference = Ytest, data = YhattestTree)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```

Random Forest
```{r}
set.seed(123)
tuneGridRanger = data.frame(splitrule = 'gini',min.node.size = c(0,5,10,15,20,25),
                            mtry = round(sqrt(ncol(XtrainMat))))
rfOut = train(x = XtrainMat, y = YtrainRelevel,
              method = "ranger",
              num.trees = 500,
              tuneGrid = tuneGridRanger,
              metric = 'Accuracy',
              trControl = trainControl)

plot(rfOut)
rfOut$bestTune

YhattestRF = predict(rfOut, XtestMat)
#confusion matrix
confusionMatrixOut = confusionMatrix(reference = Ytest, data = YhattestRF)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])

```


Boosting
```{r}
set.seed(123)
tuneGrid = data.frame('nrounds'=c(50,150,500,1000,2000,3000),
                      'max_depth' = 4,
                      'eta' = .1,
                      'gamma' = 0,
                      'colsample_bytree' = 1,
                      'min_child_weight' = 0,
                      'subsample' = 0.25)
boostOut = train(x = XtrainMat, y = YtrainRelevel,
                 method = "xgbTree",
                 tuneGrid = tuneGrid,
                 metric = 'Accuracy',
                 trControl = trainControl)
plot(boostOut)
boostOut$results
boostOut$bestTune

Yhattestboost = predict(boostOut, XtestMat)
#confusion matrix
confusionMatrixOut = confusionMatrix(reference = Ytest, data = Yhattestboost)
confusionMatrixOut$table
print(confusionMatrixOut$overall[1:2])
print(confusionMatrixOut$byClass[1:2])
```



# Results

# Conclusion







